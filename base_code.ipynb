{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import os\n",
    "import mne\n",
    "import pandas as pd\n",
    "from itertools import compress\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import re\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn import metrics\n",
    "\n",
    "# import the data here (Sai)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] The system cannot find the path specified: 'C:/Users/takoy/fnirs-github/neurotech_fnir_mind/data'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 8\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#convert the data file from NIRs to CSV\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m#start with 50 for hbo and 30 for hbr\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m#find std for hbo and hbr separately\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m#then add std to hbo 50 and hbr 30\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# preprocess the data values (worked on by Thomas and Ian)\u001b[39;00m\n\u001b[0;32m      7\u001b[0m data_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mC:/Users/takoy/fnirs-github/neurotech_fnir_mind/data\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_path\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m      9\u001b[0m     file_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(data_path, file)\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m file\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.snirf\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 3] The system cannot find the path specified: 'C:/Users/takoy/fnirs-github/neurotech_fnir_mind/data'"
     ]
    }
   ],
   "source": [
    "\n",
    "#convert the data file from NIRs to CSV\n",
    "\n",
    "#start with 50 for hbo and 30 for hbr\n",
    "#find std for hbo and hbr separately\n",
    "#then add std to hbo 50 and hbr 30\n",
    "# preprocess the data values (worked on by Thomas and Ian)\n",
    "data_path = r'C:\\Users\\takoy\\Documents\\mind-tbi-fnirs\\neurotech_fnir_mind\\data'\n",
    "for file in os.listdir(data_path):\n",
    "    file_path = os.path.join(data_path, file)\n",
    "    if not file.endswith(\".snirf\"):\n",
    "        continue\n",
    "    print(file_path)\n",
    "    raw_intensity = mne.io.read_raw_snirf(file_path, verbose=True)\n",
    "    raw_intensity.load_data()\n",
    "\n",
    "    picks = mne.pick_types(raw_intensity.info, meg=False, fnirs=True)\n",
    "    dists = mne.preprocessing.nirs.source_detector_distances(\n",
    "        raw_intensity.info, picks = picks\n",
    "    )\n",
    "    raw_intensity.pick(picks[dists > 0.01])\n",
    "    \n",
    "    raw_od = mne.preprocessing.nirs.optical_density(raw_intensity)\n",
    "\n",
    "    sci = mne.preprocessing.nirs.scalp_coupling_index(raw_od)\n",
    "\n",
    "    raw_od.info['bads'] = list(compress(raw_od.ch_names, sci < 0.5))\n",
    "\n",
    "    raw_haemo = mne.preprocessing.nirs.beer_lambert_law(raw_od, ppf=0.1)\n",
    "\n",
    "    raw_haemo_unfiltered = raw_haemo.copy()\n",
    "    raw_haemo.filter(0.05, 0.7, h_trans_bandwidth=0.2, l_trans_bandwidth=0.02)\n",
    "\n",
    "    stim_ch = np.zeros((1, raw_haemo.n_times))\n",
    "\n",
    "    #insert events manually here?\n",
    "    sfreq = raw_haemo.info['sfreq']  # Get sampling rate\n",
    "    event_interval = int(5 * sfreq)  # One event every 10 seconds\n",
    "    num_events = (raw_haemo.n_times // event_interval)  # Total possible events\n",
    "\n",
    "    manual_events = np.arange(event_interval, raw_haemo.n_times, event_interval)  # Generate event times\n",
    "\n",
    "    for e in manual_events:\n",
    "        stim_ch[0, e] = 1\n",
    "\n",
    "    info = mne.create_info(['stim'], raw_haemo.info['sfreq'], ch_types='stim')\n",
    "    stim_raw = mne.io.RawArray(stim_ch, info)\n",
    "    raw_haemo.add_channels([stim_raw], force_update_info=True)\n",
    "\n",
    "    #debugging\n",
    "    print(raw_haemo.get_data().shape)\n",
    "\n",
    "    events = mne.find_events(raw_haemo, stim_channel='stim', min_duration=0.01)\n",
    "    event_dict = {\"Tap\": 1}\n",
    "\n",
    "    hbo_std = np.std(raw_haemo.get_data(picks=\"hbo\"))\n",
    "    hbr_std = np.std(raw_haemo.get_data(picks=\"hbr\"))\n",
    "\n",
    "    reject_criteria = dict(hbo=50e-6 + hbo_std, hbr=30e-6 + hbr_std)\n",
    "    tmin, tmax = -5, 15\n",
    "\n",
    "    epochs = mne.Epochs(\n",
    "        raw_haemo,\n",
    "        events,\n",
    "        event_id=event_dict,\n",
    "        tmin=tmin,\n",
    "        tmax=tmax,\n",
    "        reject=reject_criteria,\n",
    "        reject_by_annotation=True,\n",
    "        proj=True,\n",
    "        baseline=(None, 0),\n",
    "        preload=True,\n",
    "        detrend=None,\n",
    "        verbose=True,\n",
    "    )\n",
    "\n",
    "    print(\"drop log here ##############################\")\n",
    "    epochs.plot_drop_log()\n",
    "\n",
    "    if (len(epochs) == 0):\n",
    "        print(\"no epochs found...\")\n",
    "        continue\n",
    "    evoked_dict = {\n",
    "        \"Tapping/HbO\": epochs[\"Tap\"].average(picks=\"hbo\"),\n",
    "        \"Tapping/HbR\": epochs[\"Tap\"].average(picks=\"hbr\"),\n",
    "    }\n",
    "\n",
    "    # Rename channels until the encoding of frequency in ch_name is fixed\n",
    "    for condition in evoked_dict:\n",
    "        evoked_dict[condition].rename_channels(lambda x: x[:-4])\n",
    "\n",
    "    #color_dict = dict(HbO=\"#AA3377\", HbR=\"b\")\n",
    "    #styles_dict = dict(Control=dict(linestyle=\"dashed\"))\n",
    "\n",
    "    if evoked_dict:\n",
    "        mne.viz.plot_compare_evokeds(\n",
    "            #evoked_dict, combine=\"mean\", ci=0.95, colors=color_dict, styles=styles_dict\n",
    "            evoked_dict, combine=\"mean\", ci=0.95,\n",
    "        )\n",
    "    else:\n",
    "        print(\"No valid epochs found for plotting.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "cannot unpack non-iterable NoneType object",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Split data into training and test sets\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m X_train, X_test, y_train, y_test \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m#X_train, X_test, y_train, y_test = train_test_split(, , test_size=0.2, random_state=42)\u001b[39;00m\n\u001b[0;32m      5\u001b[0m model \u001b[38;5;241m=\u001b[39m LinearRegression()\n",
      "\u001b[1;31mTypeError\u001b[0m: cannot unpack non-iterable NoneType object"
     ]
    }
   ],
   "source": [
    "# Split data into training and test sets\n",
    "X_train, X_test, y_train, y_test = None\n",
    "\n",
    "#X_train, X_test, y_train, y_test = train_test_split(, , test_size=0.2, random_state=42)\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions:\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "#Get data from fNIRs (Elan)\n",
    "\n",
    "# calculating the mean values.\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "cm = metrics.confusion_matrix(y_test, y_pred)\n",
    "\n",
    "print(\"Mean Squared Error:\", mse)\n",
    "print(\"R-squared:\", r2)\n",
    "plt.scatter(X_test, y_test, color='blue')\n",
    "plt.plot(X_test, y_pred, color='red', linewidth=2)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
