{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import os\n",
    "import mne\n",
    "import pandas as pd\n",
    "from itertools import compress\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import re\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn import metrics\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\takoy\\Documents\\mind-tbi-fnirs\\neurotech_fnir_mind\\data\\rest15.snirf\n",
      "Loading C:\\Users\\takoy\\Documents\\mind-tbi-fnirs\\neurotech_fnir_mind\\data\\rest15.snirf\n",
      "Reading 0 ... 22949  =      0.000 ...   458.980 secs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\takoy\\AppData\\Local\\Temp\\ipykernel_21808\\3383588010.py:32: RuntimeWarning: The data only contains 2D location information for the optode positions. It is highly recommended that data is used which contains 3D location information for the optode positions. With only 2D locations it can not be guaranteed that MNE functions will behave correctly and produce accurate results. If it is not possible to include 3D positions in your data, please consider using the set_montage() function.\n",
      "  raw_intensity = mne.io.read_raw_snirf(file_path, verbose=True)\n",
      "C:\\Users\\takoy\\AppData\\Local\\Temp\\ipykernel_21808\\3383588010.py:45: RuntimeWarning: Negative intensities encountered. Setting to abs(x)\n",
      "  raw_od = mne.preprocessing.nirs.optical_density(raw_intensity)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-pass filter from 0.05 - 0.7 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 0.05\n",
      "- Lower transition bandwidth: 0.02 Hz (-6 dB cutoff frequency: 0.04 Hz)\n",
      "- Upper passband edge: 0.70 Hz\n",
      "- Upper transition bandwidth: 0.20 Hz (-6 dB cutoff frequency: 0.80 Hz)\n",
      "- Filter length: 8251 samples (165.020 s)\n",
      "\n",
      "Creating RawArray with float64 data, n_channels=1, n_times=22950\n",
      "    Range : 0 ... 22949 =      0.000 ...   458.980 secs\n",
      "Ready.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  17 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done  71 tasks      | elapsed:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape after adding stim channel: (73, 22950)\n",
      "61500 22950 22950\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "All arrays must be of the same length",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 116\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(time_vector), \u001b[38;5;28mlen\u001b[39m(avg_hbo), \u001b[38;5;28mlen\u001b[39m(avg_hbr))\n\u001b[0;32m    114\u001b[0m \u001b[38;5;66;03m# Create a DataFrame that will be saved as CSV.\u001b[39;00m\n\u001b[0;32m    115\u001b[0m \u001b[38;5;66;03m# The DataFrame contains the time (relative to window start) and the averaged HbO and HbR values.\u001b[39;00m\n\u001b[1;32m--> 116\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\n\u001b[0;32m    117\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mTime (s)\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtime_vector\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    118\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mHbO_avg\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mavg_hbo\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    119\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mHbR_avg\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mavg_hbr\u001b[49m\n\u001b[0;32m    120\u001b[0m \u001b[43m\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    122\u001b[0m \u001b[38;5;66;03m# Save the DataFrame to a CSV file in the same folder\u001b[39;00m\n\u001b[0;32m    123\u001b[0m out_csv \u001b[38;5;241m=\u001b[39m file\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.snirf\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_averaged.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\pandas\\core\\frame.py:778\u001b[0m, in \u001b[0;36mDataFrame.__init__\u001b[1;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[0;32m    772\u001b[0m     mgr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_mgr(\n\u001b[0;32m    773\u001b[0m         data, axes\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mindex\u001b[39m\u001b[38;5;124m\"\u001b[39m: index, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m: columns}, dtype\u001b[38;5;241m=\u001b[39mdtype, copy\u001b[38;5;241m=\u001b[39mcopy\n\u001b[0;32m    774\u001b[0m     )\n\u001b[0;32m    776\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, \u001b[38;5;28mdict\u001b[39m):\n\u001b[0;32m    777\u001b[0m     \u001b[38;5;66;03m# GH#38939 de facto copy defaults to False only in non-dict cases\u001b[39;00m\n\u001b[1;32m--> 778\u001b[0m     mgr \u001b[38;5;241m=\u001b[39m \u001b[43mdict_to_mgr\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtyp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmanager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    779\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, ma\u001b[38;5;241m.\u001b[39mMaskedArray):\n\u001b[0;32m    780\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mma\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m mrecords\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\pandas\\core\\internals\\construction.py:503\u001b[0m, in \u001b[0;36mdict_to_mgr\u001b[1;34m(data, index, columns, dtype, typ, copy)\u001b[0m\n\u001b[0;32m    499\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    500\u001b[0m         \u001b[38;5;66;03m# dtype check to exclude e.g. range objects, scalars\u001b[39;00m\n\u001b[0;32m    501\u001b[0m         arrays \u001b[38;5;241m=\u001b[39m [x\u001b[38;5;241m.\u001b[39mcopy() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(x, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m x \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m arrays]\n\u001b[1;32m--> 503\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43marrays_to_mgr\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrays\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtyp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtyp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconsolidate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\pandas\\core\\internals\\construction.py:114\u001b[0m, in \u001b[0;36marrays_to_mgr\u001b[1;34m(arrays, columns, index, dtype, verify_integrity, typ, consolidate)\u001b[0m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m verify_integrity:\n\u001b[0;32m    112\u001b[0m     \u001b[38;5;66;03m# figure out the index, if necessary\u001b[39;00m\n\u001b[0;32m    113\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m index \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 114\u001b[0m         index \u001b[38;5;241m=\u001b[39m \u001b[43m_extract_index\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrays\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    116\u001b[0m         index \u001b[38;5;241m=\u001b[39m ensure_index(index)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\pandas\\core\\internals\\construction.py:677\u001b[0m, in \u001b[0;36m_extract_index\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m    675\u001b[0m lengths \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mset\u001b[39m(raw_lengths))\n\u001b[0;32m    676\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(lengths) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m--> 677\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAll arrays must be of the same length\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    679\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m have_dicts:\n\u001b[0;32m    680\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    681\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMixing dicts with non-Series may lead to ambiguous ordering.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    682\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: All arrays must be of the same length"
     ]
    }
   ],
   "source": [
    "# Define the data path\n",
    "data_path = r'C:\\Users\\takoy\\Documents\\mind-tbi-fnirs\\neurotech_fnir_mind\\data'\n",
    "\n",
    "# Define stimulation condition: 'real' for 20 min stimulation or 'sham' for 30 s stimulation.\n",
    "# In both cases, a fade-off period of 30 seconds is applied at the end.\n",
    "\n",
    "#yeah I don't think the files follow the study\n",
    "#files are about 7.5 mins each\n",
    "condition = 'real'  # Change to 'sham' if needed\n",
    "fade_duration_sec = 30  # Fade-off duration (seconds)\n",
    "\n",
    "if condition == 'real':\n",
    "    stim_on_duration_sec = 20 * 60  # 20 minutes in seconds\n",
    "elif condition == 'sham':\n",
    "    stim_on_duration_sec = 30       # 30 seconds\n",
    "else:\n",
    "    raise ValueError(\"Condition must be 'real' or 'sham'.\")\n",
    "\n",
    "# Total stimulation period includes the fade-off period\n",
    "#total_stim_duration_sec = stim_on_duration_sec + fade_duration_sec\n",
    "\n",
    "# Define window length for segmentation and averaging.\n",
    "#window_length_sec = total_stim_duration_sec  \n",
    "\n",
    "for file in os.listdir(data_path):\n",
    "    file_path = os.path.join(data_path, file)\n",
    "    if not file.endswith(\".snirf\"):\n",
    "        continue\n",
    "    print(file_path)\n",
    "\n",
    "    # Read the SNIRF file and load data\n",
    "    raw_intensity = mne.io.read_raw_snirf(file_path, verbose=True)\n",
    "    raw_intensity.load_data()\n",
    "\n",
    "    total_stim_duration_sec = raw_intensity.n_times / raw_intensity.info['sfreq']\n",
    "\n",
    "    # Select fNIRS channels and filter out channels with too short source-detector distances\n",
    "    picks = mne.pick_types(raw_intensity.info, meg=False, fnirs=True)\n",
    "    dists = mne.preprocessing.nirs.source_detector_distances(\n",
    "        raw_intensity.info, picks=picks\n",
    "    )\n",
    "    raw_intensity.pick(picks[dists > 0.01])\n",
    "    \n",
    "    # Convert intensity data to optical density\n",
    "    raw_od = mne.preprocessing.nirs.optical_density(raw_intensity)\n",
    "\n",
    "    # Compute scalp coupling index and mark channels with low coupling as bad\n",
    "    sci = mne.preprocessing.nirs.scalp_coupling_index(raw_od)\n",
    "    raw_od.info['bads'] = list(compress(raw_od.ch_names, sci < 0.5))\n",
    "\n",
    "    # Convert optical density to haemoglobin concentrations using the Beer-Lambert Law\n",
    "    raw_haemo = mne.preprocessing.nirs.beer_lambert_law(raw_od, ppf=0.1)\n",
    "    raw_haemo_unfiltered = raw_haemo.copy()\n",
    "\n",
    "    # Filter the haemoglobin data\n",
    "    raw_haemo.filter(0.05, 0.7, h_trans_bandwidth=0.2, l_trans_bandwidth=0.02)\n",
    "\n",
    "    # Create a binary stimulus channel based on the actual stimulation period\n",
    "    stim_ch = np.zeros((1, raw_haemo.n_times))\n",
    "    sfreq = raw_haemo.info['sfreq']\n",
    "\n",
    "    # Define stimulation start time (in seconds); adjust if there is a baseline period\n",
    "    stim_start_sec = 0  \n",
    "    stim_end_sec = stim_start_sec + total_stim_duration_sec\n",
    "\n",
    "    # Convert stimulation start and end times to sample indices\n",
    "    stim_start_index = int(stim_start_sec * sfreq)\n",
    "    stim_end_index = int(stim_end_sec * sfreq)\n",
    "\n",
    "    # Mark the stimulation period (including fade-off) with 1's; elsewhere remains 0\n",
    "    stim_ch[0, stim_start_index:stim_end_index] = 1\n",
    "\n",
    "    # Create an MNE RawArray for the stimulus channel and add it to the haemoglobin data\n",
    "    stim_info = mne.create_info(['stim'], sfreq, ch_types=['stim'])\n",
    "    stim_raw = mne.io.RawArray(stim_ch, stim_info)\n",
    "    raw_haemo.add_channels([stim_raw], force_update_info=True)\n",
    "\n",
    "    print(\"Data shape after adding stim channel:\", raw_haemo.get_data().shape)\n",
    "\n",
    "    # We use only the portion where the stim channel is 1.\n",
    "    #window_samples = int(total_stim_duration_sec * sfreq)\n",
    "    #stim_data_length = stim_end_index - stim_start_index\n",
    "    #n_windows = stim_data_length // window_samples\n",
    "    #if n_windows == 0:\n",
    "        #print(\"Not enough data for even one window in file:\", file)\n",
    "        #continue\n",
    "\n",
    "    # Initialize lists to collect segments for HbO and HbR channels\n",
    "    segments_hbo = []\n",
    "    segments_hbr = []\n",
    "    \n",
    "    # Extract non-overlapping segments from the stimulation period\n",
    "    for i in range(n_windows):\n",
    "        seg_start = stim_start_index + i * window_samples\n",
    "        seg_end = seg_start + window_samples\n",
    "        seg = raw_haemo.get_data(picks=[\"hbo\", \"hbr\"], start=seg_start, stop=seg_end)\n",
    "        # seg shape: (2, window_samples) where index 0 is HbO and 1 is HbR\n",
    "        segments_hbo.append(seg[0, :])\n",
    "        segments_hbr.append(seg[1, :])\n",
    "    \n",
    "    # Convert list of segments to numpy arrays with shape (n_windows, window_samples)\n",
    "    segments_hbo = np.array(segments_hbo)\n",
    "    segments_hbr = np.array(segments_hbr)\n",
    "    \n",
    "    # Compute the average waveform (across windows) for each channel\n",
    "    avg_hbo = np.mean(segments_hbo, axis=0)\n",
    "    avg_hbr = np.mean(segments_hbr, axis=0)\n",
    "    \n",
    "    # Create a time vector (in seconds) for one window\n",
    "    #time_vector = np.linspace(0, window_length_sec, num=window_samples, endpoint=False)\n",
    "    time_vector = np.linspace(0, total_stim_duration_sec, num=1, endpoint=False)\n",
    "\n",
    "    print(len(time_vector), len(avg_hbo), len(avg_hbr))\n",
    "\n",
    "    # Create a DataFrame that will be saved as CSV.\n",
    "    # The DataFrame contains the time (relative to window start) and the averaged HbO and HbR values.\n",
    "    df = pd.DataFrame({\n",
    "        \"Time (s)\": time_vector,\n",
    "        \"HbO_avg\": avg_hbo,\n",
    "        \"HbR_avg\": avg_hbr\n",
    "    })\n",
    "    \n",
    "    # Save the DataFrame to a CSV file in the same folder\n",
    "    out_csv = file.replace(\".snirf\", \"_averaged.csv\")\n",
    "    out_csv_path = os.path.join(data_path, out_csv)\n",
    "    df.to_csv(out_csv_path, index=False)\n",
    "    print(\"Saved averaged data CSV to:\", out_csv_path)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "cannot unpack non-iterable NoneType object",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Split data into training and test sets\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m X_train, X_test, y_train, y_test \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m#X_train, X_test, y_train, y_test = train_test_split(, , test_size=0.2, random_state=42)\u001b[39;00m\n\u001b[0;32m      5\u001b[0m model \u001b[38;5;241m=\u001b[39m LinearRegression()\n",
      "\u001b[1;31mTypeError\u001b[0m: cannot unpack non-iterable NoneType object"
     ]
    }
   ],
   "source": [
    "# Split data into training and test sets\n",
    "X_train, X_test, y_train, y_test = None\n",
    "\n",
    "#X_train, X_test, y_train, y_test = train_test_split(, , test_size=0.2, random_state=42)\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions:\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "#Get data from fNIRs (Elan)\n",
    "\n",
    "# calculating the mean values.\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "cm = metrics.confusion_matrix(y_test, y_pred)\n",
    "\n",
    "print(\"Mean Squared Error:\", mse)\n",
    "print(\"R-squared:\", r2)\n",
    "plt.scatter(X_test, y_test, color='blue')\n",
    "plt.plot(X_test, y_pred, color='red', linewidth=2)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
